{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Learning With Pytorch\n",
    "\n",
    "Hello guys, today I have a pretty interesting framework to talk about. This framework will allow us to classify images which we have never seen during the training. \n",
    "\n",
    "Last year I worked in One-Shot Learning (ONE) [1] to predict similarity between Time Series. To do so, we implemented a Siamese Neural Network [1] that was able to compute features from two different time series at a time, and later compare them using a simple L1 distance.\n",
    "\n",
    "But today I don't want to talk about ONE, I want to talk about a more powerful framework called Zero-Shot Learning (ZSL), which, as I commented before, it can classify unseen images.\n",
    "\n",
    "## How ZSL works\n",
    "\n",
    "To explain in a high level how ZSL I want to mention a quora answer [2] which I find useful and comprehensive. \n",
    " \n",
    "> What if I show you an image of an animal, given you have never seen that animal before, can you guess the name of the animal? Maybe, if you have somewhere read about that particular animal. Let's say I show a picture of zebra to a child who has never seen a zebra but has seen a horse and also she is taught that a zebra looks like a horse but with stripes. Can she identify it now? Most probably, yes!\n",
    "\n",
    "This assumption may rings a bell to you when talking about Natural Language Processing (NLP) and *word-embeddings*. With *word-embeddings* we can represent a word using a vector. If two words have a similar semantic, then the vectors will be pretty close on their vector space [3].\n",
    "\n",
    "> Let's consider a word embedding of three dimensions (although in practice the embedding varies from 100 to 300-dimensional vectors). Let these three dimensions represent features like stripes, animalness, and whiteness. So for a tiger, it would be [1, 1, 0] i.e a tiger has stripes, it is an animal but is not white in colour and for a rabbit it would be [0, 1, 1] as a rabbit does not have stripes, but is an animal and white in colour.\n",
    "\n",
    "Now coming back to CNNs, we can represent an image using a set of features, also known as a **feature vector**. A straight forward way to create a feature vector from an image is using a pretrained network.\n",
    "\n",
    "For instance, we can retrieve a feature vector from an image using PyTorch with few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Random image feature vector: torch.Size([1024])\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as zoo\n",
    "\n",
    "image = torch.randn((1, 3, 224, 224))\n",
    "model = zoo.densenet121(pretrained=True)\n",
    "feature_vector = F.adaptive_avg_pool2d(model.features(image), (1, 1))\n",
    "print('Random image feature vector:', feature_vector.view(-1).size()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for ZSL tasks, we won't use the feature vectors to classify the images into labels, we are going to use those features to match the feature vectors to a semantic representation of the label, for example, a word embedding. \n",
    "\n",
    "So, if we have an image of a tiger and his corresponding word embedding [1, 1, 0], we are going to do a regression task in order the label representation gets closer to the feature vector. And later, when we feed a zebra image (unseen image during training) to the pretrained CNN, we will be able to do a search task to the word embedding space and get the closest one. *So we were able to identify an image of a zebra which we didn't have in our training data, but had a word embedding for it.*\n",
    "\n",
    "## ZSL Framework definition\n",
    "\n",
    "At training time we have a dataset $D$ with $N$ training samples. Then, the dataset is given by $D = \\{(I_i, y^u_i, t^u_i), i = 1, ..., N\\}$ with associated labels $\\tau$, where $I_i$ is the $i$th training image, $y^u_i$ is the corresponding $L$-Dimensional semantic representation (word embedding, text description, etc.), $t^u_i \\in \\tau$ is the $u$th training class for $i$th image. \n",
    "\n",
    "Therefore, given a new test image $I_j$, the goal of ZSL is to predict a class label $t^u_j \\in \\tau'$, where $\\tau \\cap \\tau' = \\emptyset $. Meaning that both images and its label are not seen during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "sys.path.append('..')\n",
    "import zsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = 'ResNet101'\n",
    "ds = zsl.data.AwAFeaturesDataset(\n",
    "    '../data/AwA2/Animals_with_Attributes2',\n",
    "    features_type=fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antelope\n",
      "torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "features, target, semantic = ds[0]\n",
    "print(ds.classes[target])\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = zsl.data.AwAFeaturesDataset(\n",
    "    '../data/AwA2/Animals_with_Attributes2',\n",
    "    features_type=fe,\n",
    "    load_unseen=False)\n",
    "\n",
    "valid_ds = zsl.data.AwAFeaturesDataset(\n",
    "    '../data/AwA2/Animals_with_Attributes2',\n",
    "    features_type=fe,\n",
    "    load_unseen=True,\n",
    "    load_only_unseen=True)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds, \n",
    "    shuffle=True, \n",
    "    batch_size=64,\n",
    "    collate_fn=zsl.utils.collate_image_folder)\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    valid_ds, \n",
    "    batch_size=32,\n",
    "    collate_fn=zsl.utils.collate_image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7913, 29409)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_ds), len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_unit = zsl.models.LinearSemanticUnit(\n",
    "    in_features=len(train_ds.attrs),\n",
    "    out_features=1024)\n",
    "visual_fe = zsl.models.Identity(features.size(0))                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic repr shape: torch.Size([1, 1024])\n",
      "Visual embedding shape: torch.Size([1, 2048])\n"
     ]
    }
   ],
   "source": [
    "semantic = torch.FloatTensor(semantic)\n",
    "print('Semantic repr shape:', semantic_unit(semantic.unsqueeze(0)).size())\n",
    "print('Visual embedding shape:', visual_fe(features.unsqueeze(0)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2048]), torch.Size([1, 2048]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs_model = zsl.models.ZeroShot(visual_fe, semantic_unit)\n",
    "image_embed, semantic_embed = zs_model(\n",
    "    features.unsqueeze(0), torch.FloatTensor(semantic).unsqueeze(0))\n",
    "image_embed.size(), semantic_embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 2048]), torch.Size([64, 2048]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, labels, semantics = next(iter(train_dl))\n",
    "images_embeds, semantics_embeds = zs_model(features.to(device), \n",
    "                                           semantics.to(device))\n",
    "\n",
    "images_embeds.size(), semantics_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [p for p in zs_model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(parameters, 1e-4, weight_decay=9e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] [299/460] loss: 0.4748\n",
      "Epoch [0] [460/460] loss: 0.4499\n",
      "Validation accuracy: 0.2174 top_5_accuracy: 0.8322 loss: 0.5206\n",
      "Epoch [1] [299/460] loss: 0.3927\n",
      "Epoch [1] [460/460] loss: 0.3883\n",
      "Validation accuracy: 0.2709 top_5_accuracy: 0.8625 loss: 0.5146\n",
      "Epoch [2] [299/460] loss: 0.3771\n",
      "Epoch [2] [460/460] loss: 0.3737\n",
      "Validation accuracy: 0.2932 top_5_accuracy: 0.8606 loss: 0.5131\n",
      "Epoch [3] [299/460] loss: 0.3677\n",
      "Epoch [3] [460/460] loss: 0.3655\n",
      "Validation accuracy: 0.3029 top_5_accuracy: 0.8659 loss: 0.5123\n",
      "Epoch [4] [299/460] loss: 0.3623\n",
      "Epoch [4] [460/460] loss: 0.3599\n",
      "Validation accuracy: 0.3146 top_5_accuracy: 0.8705 loss: 0.5112\n",
      "Epoch [5] [299/460] loss: 0.3584\n",
      "Epoch [5] [460/460] loss: 0.3557\n",
      "Validation accuracy: 0.3249 top_5_accuracy: 0.8783 loss: 0.5113\n",
      "Epoch [6] [299/460] loss: 0.3534\n",
      "Epoch [6] [460/460] loss: 0.3527\n",
      "Validation accuracy: 0.3244 top_5_accuracy: 0.8771 loss: 0.5109\n",
      "Epoch [7] [299/460] loss: 0.3517\n",
      "Epoch [7] [460/460] loss: 0.3500\n",
      "Validation accuracy: 0.3215 top_5_accuracy: 0.8827 loss: 0.5093\n",
      "Epoch [8] [299/460] loss: 0.3490\n",
      "Epoch [8] [460/460] loss: 0.3480\n",
      "Validation accuracy: 0.3320 top_5_accuracy: 0.8880 loss: 0.5082\n",
      "Epoch [9] [299/460] loss: 0.3477\n",
      "Epoch [9] [460/460] loss: 0.3469\n",
      "Validation accuracy: 0.3239 top_5_accuracy: 0.8895 loss: 0.5078\n",
      "Epoch [10] [299/460] loss: 0.3475\n",
      "Epoch [10] [460/460] loss: 0.3456\n",
      "Validation accuracy: 0.3137 top_5_accuracy: 0.8961 loss: 0.5081\n",
      "Epoch [11] [299/460] loss: 0.3445\n",
      "Epoch [11] [460/460] loss: 0.3443\n",
      "Validation accuracy: 0.3200 top_5_accuracy: 0.8939 loss: 0.5083\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "semantic_repr = torch.FloatTensor(valid_ds.attr_matrix)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    zsl.engine.train_epoch(\n",
    "         model=zs_model, dl=train_dl, optimizer=optimizer, \n",
    "         epoch=epoch, print_freq=300, device=device)\n",
    "    \n",
    "    zsl.engine.evaluate(\n",
    "        model=zs_model, dl=valid_dl,\n",
    "        class_representations=semantic_repr, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Siamese Neural Networks for One-Shot Image Recognition - https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\n",
    "\n",
    "[2] Quora What is zero-shot learning? https://www.quora.com/What-is-zero-shot-learning\n",
    "\n",
    "[3] Playing with word embeddings - https://guillem96.github.io/guillem96-blog\n",
    "\n",
    "[2] Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly - https://arxiv.org/pdf/1707.00600.pdf"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bita0710ec61d024f3795d397d6ea4241eb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}